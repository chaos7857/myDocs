902.YOLO差别复盘
############################

起源于Fasterrcnn，，之前提到深度学习就是分类，这个之后出了目标检测

v1就是去从连续卷积后的特征图中预测xywhc
无论堆叠多少个卷积，每个点对应都是一个正方形感受野，对于其他形状区域不友好，（变化尺度较大）
v2的话就是多画几个框，不同初始化大小，每个点对应5种框
v3发现检测大目标行，稍微小一些的目标就不行
因为前面是拿最后一层做输出层，现在增加输出层，越往后识别更大的目标
（到现在也是3个输出层）

v4 科比去世同一天，原来的作者不干了，新作者开辟了一条道路——缝合怪
数据层面，四张图拼成一张进行训练
之前的dropout是随机点，现在吃掉一个区域
LabelSmoothing防止标签标错了，防止过拟合[0，1]-> [0.05, 0.95]
加了个注意力机制，用的是sam，空间注意力机制<---万恶始祖

至此，基本都是个人代码，非常多的问题
v5, v8, v11 是工程，没有论文，长得差不多

v6 和v7差了1周，导致没人看
v7 推理加速，训练的过程需要残差连接让训练稳定，但是会导致等待，通过固定卷积核大小，让所有模块的大小统一，然后在数值上进行了合并

为什么用3x3：一是因为堆叠起来效果好，二是英伟达对这个大小优化好

卷积后通常是BN，所以用一个卷积替代了卷积+BN

v9， 写的牛逼而已，题目是使用可编程的梯度信息，作者的写作思路可以学习
一般前几层卷积学的效果比较好
额外引入一条支路，将深层的梯度直接传递到浅层

v10, 追求的是速度，但是论文啥的基本都是先追求指标再追求速度
如果重叠度较高，选择置信度高的，也就是nms，这个在之前是属于后处理，v10则是将这个后处理提前了，在网络结构中先商量一下哪一个可能做的更好，后边就不需要再做筛选了，但是就造成了一个问题，如果这个点出现了问题，那么问题就大了去了，这也是为什么指标不高

v12做的是一个区域attention
每个点只和自己区域的点算attention
实现上就是把每个batch变成多个，计算完然后再合起来

v13提出了一个超图
边是人为得到设计的！！不是设计得到的，已知条件是有点右边才能加权
先推理出来一些存在的高级特征，然后平分特征，本质来说还是要做每个特征点的更新，超图的话不同高级特征之间有一些
作者是做了一个平均，一个最大，然后拼起来作为超图的初始特征

block+task