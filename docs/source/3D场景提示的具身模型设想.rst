3D场景提示的具身模型设想
############################

.. note::
    这个里边记录一些对近期想实现的想法的草稿

我一开始的想法是ROBOMETA，也就是基于原动作的，
但是这个想法已经有不少人做了，包括说像ROBOMIND就是先使用LLM对原动作进行理解
然后通过动作专家将动作作用到机器人，
这个对于个人来说工作量还是很大的，因为涉及到大量数据集的拆分和制作

我在想，能不能通过YOLO之类的模型，将目标以及其他物体进行识别，
所以这里最好是开放检测，这样才能获取到更多的信息，
接着不论是采用提示词的方式，还是直接使用crossattention进行融合，
理论上效果都应该要好过直接推理，
这里的话可以参考一下robobert的处理过程，这个还是值得回看的

所以现在的几个阶段目标是：

1. 再次回顾并且解读源码 robobert
2. 对视频的那个policy进行学习
3. 实现一下将检测结果融入进去
4. 可以优化一下检测的算法，不只是开放的3d检测，还有检测边框的tight

VLM 更擅长高层级的推理，而 AIGC 生成式模型更擅长细节处理